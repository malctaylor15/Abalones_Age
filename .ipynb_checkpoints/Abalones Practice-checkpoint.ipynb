{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First let's import some packages that might be useful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn as sk "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next let's look at the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sex  Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
      "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
      "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
      "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
      "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
      "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
      "\n",
      "   Shell weight  Rings  \n",
      "0         0.150     15  \n",
      "1         0.070      7  \n",
      "2         0.210      9  \n",
      "3         0.155     10  \n",
      "4         0.055      7   \n",
      " \n",
      "              Length     Diameter       Height  Whole weight  Shucked weight  \\\n",
      "count  4177.000000  4177.000000  4177.000000   4177.000000     4177.000000   \n",
      "mean      0.523992     0.407881     0.139516      0.828742        0.359367   \n",
      "std       0.120093     0.099240     0.041827      0.490389        0.221963   \n",
      "min       0.075000     0.055000     0.000000      0.002000        0.001000   \n",
      "25%       0.450000     0.350000     0.115000      0.441500        0.186000   \n",
      "50%       0.545000     0.425000     0.140000      0.799500        0.336000   \n",
      "75%       0.615000     0.480000     0.165000      1.153000        0.502000   \n",
      "max       0.815000     0.650000     1.130000      2.825500        1.488000   \n",
      "\n",
      "       Viscera weight  Shell weight        Rings  \n",
      "count     4177.000000   4177.000000  4177.000000  \n",
      "mean         0.180594      0.238831     9.933684  \n",
      "std          0.109614      0.139203     3.224169  \n",
      "min          0.000500      0.001500     1.000000  \n",
      "25%          0.093500      0.130000     8.000000  \n",
      "50%          0.171000      0.234000     9.000000  \n",
      "75%          0.253000      0.329000    11.000000  \n",
      "max          0.760000      1.005000    29.000000  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"toy_ds.csv\")\n",
    "print(data.head(), '\\n \\n ', data.describe())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Change the Sex variable to be numbers - first let's see how many unique \"Sex\"s there are \n",
    "Then we will change each one to be a number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'F' 'I']\n"
     ]
    }
   ],
   "source": [
    "print(data[\"Sex\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc[data[\"Sex\"]=='M' , \"Sex\" ] = 0\n",
    "data.loc[data[\"Sex\"]=='F' , \"Sex\" ] = 1\n",
    "data.loc[data[\"Sex\"]=='I' , \"Sex\" ] = 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want to sort based on three categories \n",
    "Young (1) - 1-8 \n",
    "Medium (2) - 9-10\n",
    "Old (3) - 11+ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgeCategory\n",
      "1    1407\n",
      "2    1323\n",
      "3    1447\n",
      "dtype: int64 \n",
      " 4177\n"
     ]
    }
   ],
   "source": [
    "data[\"AgeCategory\"] = 0\n",
    "data.loc[data['Rings'] <9, \"AgeCategory\"] = 1\n",
    "data.loc[(data['Rings'] >= 9) & (data['Rings'] <= 10), \"AgeCategory\"] = 2\n",
    "data.loc[data['Rings'] >10, \"AgeCategory\" ] = 3\n",
    "print(data.groupby('AgeCategory').size(), '\\n',\n",
    "sum(data.groupby('AgeCategory').size()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For more information about the sets and save information to a variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Length</th>\n",
       "      <th>Rings</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Whole weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AgeCategory</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">1</th>\n",
       "      <th>count</th>\n",
       "      <td>1407.000000</td>\n",
       "      <td>1407.000000</td>\n",
       "      <td>1407.000000</td>\n",
       "      <td>1407.000000</td>\n",
       "      <td>1407.000000</td>\n",
       "      <td>1407.000000</td>\n",
       "      <td>1407.000000</td>\n",
       "      <td>1407.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.321276</td>\n",
       "      <td>0.106596</td>\n",
       "      <td>0.420991</td>\n",
       "      <td>6.884151</td>\n",
       "      <td>0.121394</td>\n",
       "      <td>0.198199</td>\n",
       "      <td>0.093357</td>\n",
       "      <td>0.432374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.090292</td>\n",
       "      <td>0.041830</td>\n",
       "      <td>0.111375</td>\n",
       "      <td>1.216914</td>\n",
       "      <td>0.080965</td>\n",
       "      <td>0.147031</td>\n",
       "      <td>0.068090</td>\n",
       "      <td>0.306007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.082250</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.106500</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.368500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.169750</td>\n",
       "      <td>0.282250</td>\n",
       "      <td>0.129250</td>\n",
       "      <td>0.597000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.565000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.825500</td>\n",
       "      <td>0.385500</td>\n",
       "      <td>1.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">2</th>\n",
       "      <th>count</th>\n",
       "      <td>1323.000000</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>1323.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.436754</td>\n",
       "      <td>0.148171</td>\n",
       "      <td>0.560170</td>\n",
       "      <td>9.479214</td>\n",
       "      <td>0.258777</td>\n",
       "      <td>0.416345</td>\n",
       "      <td>0.204731</td>\n",
       "      <td>0.927122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.069540</td>\n",
       "      <td>0.029392</td>\n",
       "      <td>0.085249</td>\n",
       "      <td>0.499757</td>\n",
       "      <td>0.101876</td>\n",
       "      <td>0.194768</td>\n",
       "      <td>0.091182</td>\n",
       "      <td>0.396543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.205000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.271250</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.199000</td>\n",
       "      <td>0.917500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.322500</td>\n",
       "      <td>0.539250</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>1.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>1.253000</td>\n",
       "      <td>0.541000</td>\n",
       "      <td>2.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">3</th>\n",
       "      <th>count</th>\n",
       "      <td>1447.000000</td>\n",
       "      <td>1447.000000</td>\n",
       "      <td>1447.000000</td>\n",
       "      <td>1447.000000</td>\n",
       "      <td>1447.000000</td>\n",
       "      <td>1447.000000</td>\n",
       "      <td>1447.000000</td>\n",
       "      <td>1447.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.465695</td>\n",
       "      <td>0.163614</td>\n",
       "      <td>0.591068</td>\n",
       "      <td>13.314444</td>\n",
       "      <td>0.334784</td>\n",
       "      <td>0.463986</td>\n",
       "      <td>0.243350</td>\n",
       "      <td>1.124204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.068473</td>\n",
       "      <td>0.029277</td>\n",
       "      <td>0.083205</td>\n",
       "      <td>2.770863</td>\n",
       "      <td>0.130774</td>\n",
       "      <td>0.217678</td>\n",
       "      <td>0.103497</td>\n",
       "      <td>0.458919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.793500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>1.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>1.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>1.488000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>2.825500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Diameter       Height       Length        Rings  \\\n",
       "AgeCategory                                                             \n",
       "1           count  1407.000000  1407.000000  1407.000000  1407.000000   \n",
       "            mean      0.321276     0.106596     0.420991     6.884151   \n",
       "            std       0.090292     0.041830     0.111375     1.216914   \n",
       "            min       0.055000     0.000000     0.075000     1.000000   \n",
       "            25%       0.260000     0.085000     0.350000     6.000000   \n",
       "            50%       0.325000     0.105000     0.430000     7.000000   \n",
       "            75%       0.385000     0.125000     0.500000     8.000000   \n",
       "            max       0.565000     1.130000     0.720000     8.000000   \n",
       "2           count  1323.000000  1323.000000  1323.000000  1323.000000   \n",
       "            mean      0.436754     0.148171     0.560170     9.479214   \n",
       "            std       0.069540     0.029392     0.085249     0.499757   \n",
       "            min       0.205000     0.015000     0.280000     9.000000   \n",
       "            25%       0.395000     0.130000     0.515000     9.000000   \n",
       "            50%       0.450000     0.150000     0.575000     9.000000   \n",
       "            75%       0.485000     0.165000     0.620000    10.000000   \n",
       "            max       0.600000     0.515000     0.770000    10.000000   \n",
       "3           count  1447.000000  1447.000000  1447.000000  1447.000000   \n",
       "            mean      0.465695     0.163614     0.591068    13.314444   \n",
       "            std       0.068473     0.029277     0.083205     2.770863   \n",
       "            min       0.235000     0.060000     0.310000    11.000000   \n",
       "            25%       0.420000     0.145000     0.535000    11.000000   \n",
       "            50%       0.475000     0.165000     0.600000    12.000000   \n",
       "            75%       0.515000     0.185000     0.650000    15.000000   \n",
       "            max       0.650000     0.250000     0.815000    29.000000   \n",
       "\n",
       "                   Shell weight  Shucked weight  Viscera weight  Whole weight  \n",
       "AgeCategory                                                                    \n",
       "1           count   1407.000000     1407.000000     1407.000000   1407.000000  \n",
       "            mean       0.121394        0.198199        0.093357      0.432374  \n",
       "            std        0.080965        0.147031        0.068090      0.306007  \n",
       "            min        0.001500        0.001000        0.000500      0.002000  \n",
       "            25%        0.060000        0.082250        0.040000      0.196000  \n",
       "            50%        0.106500        0.165000        0.078500      0.368500  \n",
       "            75%        0.169750        0.282250        0.129250      0.597000  \n",
       "            max        0.470000        0.825500        0.385500      1.710000  \n",
       "2           count   1323.000000     1323.000000     1323.000000   1323.000000  \n",
       "            mean       0.258777        0.416345        0.204731      0.927122  \n",
       "            std        0.101876        0.194768        0.091182      0.396543  \n",
       "            min        0.040000        0.045500        0.026000      0.124000  \n",
       "            25%        0.185000        0.271250        0.139000      0.646000  \n",
       "            50%        0.259000        0.415000        0.199000      0.917500  \n",
       "            75%        0.322500        0.539250        0.264500      1.184000  \n",
       "            max        0.655000        1.253000        0.541000      2.381000  \n",
       "3           count   1447.000000     1447.000000     1447.000000   1447.000000  \n",
       "            mean       0.334784        0.463986        0.243350      1.124204  \n",
       "            std        0.130774        0.217678        0.103497      0.458919  \n",
       "            min        0.040000        0.041500        0.024000      0.120000  \n",
       "            25%        0.249000        0.299500        0.166000      0.793500  \n",
       "            50%        0.324500        0.437000        0.234000      1.092000  \n",
       "            75%        0.410000        0.598000        0.306000      1.410500  \n",
       "            max        1.005000        1.488000        0.760000      2.825500  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('AgeCategory').describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Drop the Column with the rings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_use = data.drop('Rings',1)\n",
    "table1 = data_use.groupby('AgeCategory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# table1['Diameter'].agg([np.mean,np.std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_FE = data_use\n",
    "data_FE[\"Volume\"] = data_use[\"Length\"]*data_use[\"Diameter\"]*data_use[\"Height\"]\n",
    "data_FE[\"Vol*Weight\"] = data_use[\"Volume\"] * data_use[\"Whole weight\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we know more about our data set and it is ready to be used... lets try a multinomial logistics regression\n",
    "Start by importing the Logisitic Regression Module from sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Split dataframe into input and output variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data_FE.drop('AgeCategory', 1)\n",
    "Y = data_FE[\"AgeCategory\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Run logistic regression in various settings... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "#OVR_fit = OneVsRestClassifier(LogisticRegression()).fit(X,Y)\n",
    "#OVO_fit = OneVsOneClassifier(LogisticRegression()).fit(X,Y)\n",
    "log_fit = LogisticRegression().fit(X,Y)\n",
    "print(log_fit)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Check the raw predictive accuracy of each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predict_OVR = OVR_fit.predict(X)\n",
    "#predict_OVO = OVO_fit.predict(X)\n",
    "predict_log = log_fit.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.67      0.79      0.73      1407\n",
      "          2       0.52      0.37      0.43      1323\n",
      "          3       0.66      0.71      0.68      1447\n",
      "\n",
      "avg / total       0.62      0.63      0.62      4177\n",
      "\n",
      "[[1116  231   60]\n",
      " [ 360  493  470]\n",
      " [ 186  233 1028]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(Y,predict_log))\n",
    "print(metrics.confusion_matrix(Y, predict_log))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Logistic regression \n",
    "Not great at 63% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(X,Y)\n",
    "predict_KNN = KNN.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.78      0.85      0.81      1407\n",
      "          2       0.64      0.66      0.65      1323\n",
      "          3       0.80      0.71      0.76      1447\n",
      "\n",
      "avg / total       0.74      0.74      0.74      4177\n",
      "\n",
      "[[1194  181   32]\n",
      " [ 226  871  226]\n",
      " [ 110  303 1034]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(Y,predict_KNN))\n",
    "print(metrics.confusion_matrix(Y, predict_KNN))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Not bad for KNN \n",
    "74 % accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(random_state=1)\n",
    "RF.fit(X,Y)\n",
    "predict_RF = RF.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.98      0.99      0.98      1407\n",
      "          2       0.98      0.97      0.97      1323\n",
      "          3       0.99      0.98      0.98      1447\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4177\n",
      "\n",
      "[[1398    5    4]\n",
      " [  25 1281   17]\n",
      " [   9   22 1416]]\n",
      "0.609780218836\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(Y,predict_RF))\n",
    "print(metrics.confusion_matrix(Y, predict_RF))\n",
    "print(cross_val_score(RF, X,Y,cv= 5).mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I think the tree is too deep and over fitting \n",
    "I am suspicious of that 98% "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Try cross using cross validation to find optimal number of trees at split \n",
    "\n",
    "Try playing with number of trees in the forest,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CV = 1\n",
    "Parameter_df = pd.DataFrame({\"Numb Features\" : 0.1 , \"Numb Trees\": 0.2, \"Min Leaf Size\":0.2, \"Node Split\": 0.3,\"CV\":0.4},\n",
    "                            index= index)\n",
    "Parameter_df.loc[1]= [num_feat, num_est, min_leaf, min_split ,CV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      "\n",
      " 2\n",
      "\n",
      " 3\n",
      "\n",
      " 4\n",
      "\n",
      " 5\n",
      "\n",
      " 6\n",
      "\n",
      " 7\n",
      "\n",
      " 8\n",
      "\n",
      " 9\n",
      "\n",
      " 10\n",
      "\n",
      " 11\n",
      "\n",
      " 12\n",
      "\n",
      " 13\n",
      "\n",
      " 14\n",
      "\n",
      " 15\n",
      "\n",
      " 16\n",
      "\n",
      " 17\n",
      "\n",
      " 18\n",
      "\n",
      " 19\n",
      "\n",
      " 20\n",
      "\n",
      " 21\n",
      "\n",
      " 22\n",
      "\n",
      " 23\n",
      "\n",
      " 24\n",
      "\n",
      " 25\n",
      "\n",
      " 26\n",
      "\n",
      " 27\n",
      "\n",
      " 28\n",
      "\n",
      " 29\n",
      "\n",
      " 30\n",
      "\n",
      " 31\n",
      "\n",
      " 32\n",
      "\n",
      " 33\n",
      "\n",
      " 34\n",
      "\n",
      " 35\n",
      "\n",
      " 36\n",
      "\n",
      " 37\n",
      "\n",
      " 38\n",
      "\n",
      " 39\n",
      "\n",
      " 40\n",
      "\n",
      " 41\n",
      "\n",
      " 42\n",
      "\n",
      " 43\n",
      "\n",
      " 44\n",
      "\n",
      " 45\n",
      "\n",
      " 46\n",
      "\n",
      " 47\n",
      "\n",
      " 48\n",
      "\n",
      " 49\n",
      "\n",
      " 50\n",
      "\n",
      " 51\n",
      "\n",
      " 52\n",
      "\n",
      " 53\n",
      "\n",
      " 54\n",
      "\n",
      " 55\n",
      "\n",
      " 56\n",
      "\n",
      " 57\n",
      "\n",
      " 58\n",
      "\n",
      " 59\n",
      "\n",
      " 60\n",
      "\n",
      " 61\n",
      "\n",
      " 62\n",
      "\n",
      " 63\n",
      "\n",
      " 64\n",
      "\n",
      " 65\n",
      "\n",
      " 66\n",
      "\n",
      " 67\n",
      "\n",
      " 68\n",
      "\n",
      " 69\n",
      "\n",
      " 70\n",
      "\n",
      " 71\n",
      "\n",
      " 72\n",
      "\n",
      " 73\n",
      "\n",
      " 74\n",
      "\n",
      " 75\n",
      "\n",
      " 76\n",
      "\n",
      " 77\n",
      "\n",
      " 78\n",
      "\n",
      " 79\n",
      "\n",
      " 80\n",
      "\n",
      " 81\n",
      "\n",
      " 82\n",
      "\n",
      " 83\n",
      "\n",
      " 84\n",
      "\n",
      " 85\n",
      "\n",
      " 86\n",
      "\n",
      " 87\n",
      "\n",
      " 88\n",
      "\n",
      " 89\n",
      "\n",
      " 90\n",
      "\n",
      " 91\n",
      "\n",
      " 92\n",
      "\n",
      " 93\n",
      "\n",
      " 94\n",
      "\n",
      " 95\n",
      "\n",
      " 96\n",
      "\n",
      " 97\n",
      "\n",
      " 98\n",
      "\n",
      " 99\n",
      "\n",
      " 100\n",
      "\n",
      " 101\n",
      "\n",
      " 102\n",
      "\n",
      " 103\n",
      "\n",
      " 104\n",
      "\n",
      " 105\n",
      "\n",
      " 106\n",
      "\n",
      " 107\n",
      "\n",
      " 108\n",
      "\n",
      " 109\n",
      "\n",
      " 110\n",
      "\n",
      " 111\n",
      "\n",
      " 112\n",
      "\n",
      " 113\n",
      "\n",
      " 114\n",
      "\n",
      " 115\n",
      "\n",
      " 116\n",
      "\n",
      " 117\n",
      "\n",
      " 118\n",
      "\n",
      " 119\n",
      "\n",
      " 120\n",
      "\n",
      " 121\n",
      "\n",
      " 122\n",
      "\n",
      " 123\n",
      "\n",
      " 124\n",
      "\n",
      " 125\n",
      "\n",
      " 126\n",
      "\n",
      " 127\n",
      "\n",
      " 128\n",
      "\n",
      " 129\n",
      "\n",
      " 130\n",
      "\n",
      " 131\n",
      "\n",
      " 132\n",
      "\n",
      " 133\n",
      "\n",
      " 134\n",
      "\n",
      " 135\n",
      "\n",
      " 136\n",
      "\n",
      " 137\n",
      "\n",
      " 138\n",
      "\n",
      " 139\n",
      "\n",
      " 140\n",
      "\n",
      " 141\n",
      "\n",
      " 142\n",
      "\n",
      " 143\n",
      "\n",
      " 144\n",
      "\n",
      " 145\n",
      "\n",
      " 146\n",
      "\n",
      " 147\n",
      "\n",
      " 148\n",
      "\n",
      " 149\n",
      "\n",
      " 150\n",
      "\n",
      " 151\n",
      "\n",
      " 152\n",
      "\n",
      " 153\n",
      "\n",
      " 154\n",
      "\n",
      " 155\n",
      "\n",
      " 156\n",
      "\n",
      " 157\n",
      "\n",
      " 158\n",
      "\n",
      " 159\n",
      "\n",
      " 160\n",
      "\n",
      " 161\n",
      "\n",
      " 162\n",
      "\n",
      " 163\n",
      "\n",
      " 164\n",
      "\n",
      " 165\n",
      "\n",
      " 166\n",
      "\n",
      " 167\n",
      "\n",
      " 168\n",
      "\n",
      " 169\n",
      "\n",
      " 170\n",
      "\n",
      " 171\n",
      "\n",
      " 172\n",
      "\n",
      " 173\n",
      "\n",
      " 174\n",
      "\n",
      " 175\n",
      "\n",
      " 176\n",
      "\n",
      " 177\n",
      "\n",
      " 178\n",
      "\n",
      " 179\n",
      "\n",
      " 180\n",
      "\n",
      " 181\n",
      "\n",
      " 182\n",
      "\n",
      " 183\n",
      "\n",
      " 184\n",
      "\n",
      " 185\n",
      "\n",
      " 186\n",
      "\n",
      " 187\n",
      "\n",
      " 188\n",
      "\n",
      " 189\n",
      "\n",
      " 190\n",
      "\n",
      " 191\n",
      "\n",
      " 192\n",
      "\n",
      " 193\n",
      "\n",
      " 194\n",
      "\n",
      " 195\n",
      "\n",
      " 196\n",
      "\n",
      " 197\n",
      "\n",
      " 198\n",
      "\n",
      " 199\n",
      "\n",
      " 200\n",
      "\n",
      " 201\n",
      "\n",
      " 202\n",
      "\n",
      " 203\n",
      "\n",
      " 204\n",
      "\n",
      " 205\n",
      "\n",
      " 206\n",
      "\n",
      " 207\n",
      "\n",
      " 208\n",
      "\n",
      " 209\n",
      "\n",
      " 210\n",
      "\n",
      " 211\n",
      "\n",
      " 212\n",
      "\n",
      " 213\n",
      "\n",
      " 214\n",
      "\n",
      " 215\n",
      "\n",
      " 216\n",
      "\n",
      " 217\n",
      "\n",
      " 218\n",
      "\n",
      " 219\n",
      "\n",
      " 220\n",
      "\n",
      " 221\n",
      "\n",
      " 222\n",
      "\n",
      " 223\n",
      "\n",
      " 224\n",
      "\n",
      " 225\n",
      "\n",
      " 226\n",
      "\n",
      " 227\n",
      "\n",
      " 228\n",
      "\n",
      " 229\n",
      "\n",
      " 230\n",
      "\n",
      " 231\n",
      "\n",
      " 232\n",
      "\n",
      " 233\n",
      "\n",
      " 234\n",
      "\n",
      " 235\n",
      "\n",
      " 236\n",
      "\n",
      " 237\n",
      "\n",
      " 238\n",
      "\n",
      " 239\n",
      "\n",
      " 240\n",
      "\n",
      " 241\n",
      "\n",
      " 242\n",
      "\n",
      " 243\n",
      "\n",
      " 244\n",
      "\n",
      " 245\n",
      "\n",
      " 246\n",
      "\n",
      " 247\n",
      "\n",
      " 248\n",
      "\n",
      " 249\n",
      "\n",
      " 250\n",
      "\n",
      " 251\n",
      "\n",
      " 252\n",
      "\n",
      " 253\n",
      "\n",
      " 254\n",
      "\n",
      " 255\n",
      "\n",
      " 256\n",
      "\n",
      " 257\n",
      "\n",
      " 258\n",
      "\n",
      " 259\n",
      "\n",
      " 260\n",
      "\n",
      " 261\n",
      "\n",
      " 262\n",
      "\n",
      " 263\n",
      "\n",
      " 264\n",
      "\n",
      " 265\n",
      "\n",
      " 266\n",
      "\n",
      " 267\n",
      "\n",
      " 268\n",
      "\n",
      " 269\n",
      "\n",
      " 270\n",
      "\n",
      " 271\n",
      "\n",
      " 272\n",
      "\n",
      " 273\n",
      "\n",
      " 274\n",
      "\n",
      " 275\n",
      "\n",
      " 276\n",
      "\n",
      " 277\n",
      "\n",
      " 278\n",
      "\n",
      " 279\n",
      "\n",
      " 280\n",
      "\n",
      " 281\n",
      "\n",
      " 282\n",
      "\n",
      " 283\n",
      "\n",
      " 284\n",
      "\n",
      " 285\n",
      "\n",
      " 286\n",
      "\n",
      " 287\n",
      "\n",
      " 288\n",
      "\n",
      " 289\n",
      "\n",
      " 290\n",
      "\n",
      " 291\n",
      "\n",
      " 292\n",
      "\n",
      " 293\n",
      "\n",
      " 294\n",
      "\n",
      " 295\n",
      "\n",
      " 296\n",
      "\n",
      " 297\n",
      "\n",
      " 298\n",
      "\n",
      " 299\n",
      "\n",
      " 300\n",
      "\n",
      " 301\n",
      "\n",
      " 302\n",
      "\n",
      " 303\n",
      "\n",
      " 304\n",
      "\n",
      " 305\n",
      "\n",
      " 306\n",
      "\n",
      " 307\n",
      "\n",
      " 308\n",
      "\n",
      " 309\n",
      "\n",
      " 310\n",
      "\n",
      " 311\n",
      "\n",
      " 312\n",
      "\n",
      " 313\n",
      "\n",
      " 314\n",
      "\n",
      " 315\n",
      "\n",
      " 316\n",
      "\n",
      " 317\n",
      "\n",
      " 318\n",
      "\n",
      " 319\n",
      "\n",
      " 320\n",
      "\n",
      " 321\n",
      "\n",
      " 322\n",
      "\n",
      " 323\n",
      "\n",
      " 324\n",
      "\n",
      " 325\n",
      "\n",
      " 326\n",
      "\n",
      " 327\n",
      "\n",
      " 328\n",
      "\n",
      " 329\n",
      "\n",
      " 330\n",
      "\n",
      " 331\n",
      "\n",
      " 332\n",
      "\n",
      " 333\n",
      "\n",
      " 334\n",
      "\n",
      " 335\n",
      "\n",
      " 336\n",
      "\n",
      " 337\n",
      "\n",
      " 338\n",
      "\n",
      " 339\n",
      "\n",
      " 340\n",
      "\n",
      " 341\n",
      "\n",
      " 342\n",
      "\n",
      " 343\n",
      "\n",
      " 344\n",
      "\n",
      " 345\n",
      "\n",
      " 346\n",
      "\n",
      " 347\n",
      "\n",
      " 348\n",
      "\n",
      " 349\n",
      "\n",
      " 350\n",
      "\n",
      " 351\n",
      "\n",
      " 352\n",
      "\n",
      " 353\n",
      "\n",
      " 354\n",
      "\n",
      " 355\n",
      "\n",
      " 356\n",
      "\n",
      " 357\n",
      "\n",
      " 358\n",
      "\n",
      " 359\n",
      "\n",
      " 360\n",
      "\n",
      " 361\n",
      "\n",
      " 362\n",
      "\n",
      " 363\n",
      "\n",
      " 364\n",
      "\n",
      " 365\n",
      "\n",
      " 366\n",
      "\n",
      " 367\n",
      "\n",
      " 368\n",
      "\n",
      " 369\n",
      "\n",
      " 370\n",
      "\n",
      " 371\n",
      "\n",
      " 372\n",
      "\n",
      " 373\n",
      "\n",
      " 374\n",
      "\n",
      " 375\n",
      "\n",
      " 376\n",
      "\n",
      " 377\n",
      "\n",
      " 378\n",
      "\n",
      " 379\n",
      "\n",
      " 380\n",
      "\n",
      " 381\n",
      "\n",
      " 382\n",
      "\n",
      " 383\n",
      "\n",
      " 384\n",
      "\n",
      " 385\n",
      "\n",
      " 386\n",
      "\n",
      " 387\n",
      "\n",
      " 388\n",
      "\n",
      " 389\n",
      "\n",
      " 390\n",
      "\n",
      " 391\n",
      "\n",
      " 392\n",
      "\n",
      " 393\n",
      "\n",
      " 394\n",
      "\n",
      " 395\n",
      "\n",
      " 396\n",
      "\n",
      " 397\n",
      "\n",
      " 398\n",
      "\n",
      " 399\n",
      "\n",
      " 400\n",
      "\n",
      " 401\n",
      "\n",
      " 402\n",
      "\n",
      " 403\n",
      "\n",
      " 404\n",
      "\n",
      " 405\n",
      "\n",
      " 406\n",
      "\n",
      " 407\n",
      "\n",
      " 408\n",
      "\n",
      " 409\n",
      "\n",
      " 410\n",
      "\n",
      " 411\n",
      "\n",
      " 412\n",
      "\n",
      " 413\n",
      "\n",
      " 414\n",
      "\n",
      " 415\n",
      "\n",
      " 416\n",
      "\n",
      " 417\n",
      "\n",
      " 418\n",
      "\n",
      " 419\n",
      "\n",
      " 420\n",
      "\n",
      " 421\n",
      "\n",
      " 422\n",
      "\n",
      " 423\n",
      "\n",
      " 424\n",
      "\n",
      " 425\n",
      "\n",
      " 426\n",
      "\n",
      " 427\n",
      "\n",
      " 428\n",
      "\n",
      " 429\n",
      "\n",
      " 430\n",
      "\n",
      " 431\n",
      "\n",
      " 432\n",
      "\n",
      " 433\n",
      "\n",
      " 434\n",
      "\n",
      " 435\n",
      "\n",
      " 436\n",
      "\n",
      " 437\n",
      "\n",
      " 438\n",
      "\n",
      " 439\n",
      "\n",
      " 440\n",
      "\n",
      " 441\n",
      "\n",
      " 442\n",
      "\n",
      " 443\n",
      "\n",
      " 444\n",
      "\n",
      " 445\n",
      "\n",
      " 446\n",
      "\n",
      " 447\n",
      "\n",
      " 448\n",
      "\n",
      " 449\n",
      "\n",
      " 450\n",
      "\n",
      " 451\n",
      "\n",
      " 452\n",
      "\n",
      " 453\n",
      "\n",
      " 454\n",
      "\n",
      " 455\n",
      "\n",
      " 456\n",
      "\n",
      " 457\n",
      "\n",
      " 458\n",
      "\n",
      " 459\n",
      "\n",
      " 460\n",
      "\n",
      " 461\n",
      "\n",
      " 462\n",
      "\n",
      " 463\n",
      "\n",
      " 464\n",
      "\n",
      " 465\n",
      "\n",
      " 466\n",
      "\n",
      " 467\n",
      "\n",
      " 468\n",
      "\n",
      " 469\n",
      "\n",
      " 470\n",
      "\n",
      " 471\n",
      "\n",
      " 472\n",
      "\n",
      " 473\n",
      "\n",
      " 474\n",
      "\n",
      " 475\n",
      "\n",
      " 476\n",
      "\n",
      " 477\n",
      "\n",
      " 478\n",
      "\n",
      " 479\n",
      "\n",
      " 480\n",
      "\n",
      " 481\n",
      "\n",
      " 482\n",
      "\n",
      " 483\n",
      "\n",
      " 484\n",
      "\n",
      " 485\n",
      "\n",
      " 486\n",
      "\n",
      " 487\n",
      "\n",
      " 488\n",
      "\n",
      " 489\n",
      "\n",
      " 490\n",
      "\n",
      " 491\n",
      "\n",
      " 492\n",
      "\n",
      " 493\n",
      "\n",
      " 494\n",
      "\n",
      " 495\n",
      "\n",
      " 496\n",
      "\n",
      " 497\n",
      "\n",
      " 498\n",
      "\n",
      " 499\n",
      "\n",
      " 500\n",
      "\n",
      " 501\n",
      "\n",
      " 502\n",
      "\n",
      " 503\n",
      "\n",
      " 504\n",
      "\n",
      " 505\n",
      "\n",
      " 506\n",
      "\n",
      " 507\n",
      "\n",
      " 508\n",
      "\n",
      " 509\n",
      "\n",
      " 510\n",
      "\n",
      " 511\n",
      "\n",
      " 512\n",
      "\n",
      " 513\n",
      "\n",
      " 514\n",
      "\n",
      " 515\n",
      "\n",
      " 516\n",
      "\n",
      " 517\n",
      "\n",
      " 518\n",
      "\n",
      " 519\n",
      "\n",
      " 520\n",
      "\n",
      " 521\n",
      "\n",
      " 522\n",
      "\n",
      " 523\n",
      "\n",
      " 524\n",
      "\n",
      " 525\n",
      "\n",
      " 526\n",
      "\n",
      " 527\n",
      "\n",
      " 528\n",
      "\n",
      " 529\n",
      "\n",
      " 530\n",
      "\n",
      " 531\n",
      "\n",
      " 532\n",
      "\n",
      " 533\n",
      "\n",
      " 534\n",
      "\n",
      " 535\n",
      "\n",
      " 536\n",
      "\n",
      " 537\n",
      "\n",
      " 538\n",
      "\n",
      " 539\n",
      "\n",
      " 540\n",
      "\n",
      " 541\n",
      "\n",
      " 542\n",
      "\n",
      " 543\n",
      "\n",
      " 544\n",
      "\n",
      " 545\n",
      "\n",
      " 546\n",
      "\n",
      " 547\n",
      "\n",
      " 548\n",
      "\n",
      " 549\n",
      "\n",
      " 550\n",
      "\n",
      " 551\n",
      "\n",
      " 552\n",
      "\n",
      " 553\n",
      "\n",
      " 554\n",
      "\n",
      " 555\n",
      "\n",
      " 556\n",
      "\n",
      " 557\n",
      "\n",
      " 558\n",
      "\n",
      " 559\n",
      "\n",
      " 560\n",
      "\n",
      " 561\n",
      "\n",
      " 562\n",
      "\n",
      " 563\n",
      "\n",
      " 564\n",
      "\n",
      " 565\n",
      "\n",
      " 566\n",
      "\n",
      " 567\n",
      "\n",
      " 568\n",
      "\n",
      " 569\n",
      "\n",
      " 570\n",
      "\n",
      " 571\n",
      "\n",
      " 572\n",
      "\n",
      " 573\n",
      "\n",
      " 574\n",
      "\n",
      " 575\n",
      "\n",
      " 576\n",
      "\n",
      " 577\n",
      "\n",
      " 578\n",
      "\n",
      " 579\n",
      "\n",
      " 580\n",
      "\n",
      " 581\n",
      "\n",
      " 582\n",
      "\n",
      " 583\n",
      "\n",
      " 584\n",
      "\n",
      " 585\n",
      "\n",
      " 586\n",
      "\n",
      " 587\n",
      "\n",
      " 588\n",
      "\n",
      " 589\n",
      "\n",
      " 590\n",
      "\n",
      " 591\n",
      "\n",
      " 592\n",
      "\n",
      " 593\n",
      "\n",
      " 594\n",
      "\n",
      " 595\n",
      "\n",
      " 596\n",
      "\n",
      " 597\n",
      "\n",
      " 598\n",
      "\n",
      " 599\n",
      "\n",
      " 600\n",
      "\n",
      " 601\n",
      "\n",
      " 602\n",
      "\n",
      " 603\n",
      "\n",
      " 604\n",
      "\n",
      " 605\n",
      "\n",
      " 606\n",
      "\n",
      " 607\n",
      "\n",
      " 608\n",
      "\n",
      " 609\n",
      "\n",
      " 610\n",
      "\n",
      " 611\n",
      "\n",
      " 612\n",
      "\n",
      " 613\n",
      "\n",
      " 614\n",
      "\n",
      " 615\n",
      "\n",
      " 616\n",
      "\n",
      " 617\n",
      "\n",
      " 618\n",
      "\n",
      " 619\n",
      "\n",
      " 620\n",
      "\n",
      " 621\n",
      "\n",
      " 622\n",
      "\n",
      " 623\n",
      "\n",
      " 624\n",
      "\n",
      " 625\n",
      "\n",
      " 626\n",
      "\n",
      " 627\n",
      "\n",
      " 628\n",
      "\n",
      " 629\n",
      "\n",
      " 630\n",
      "\n",
      " 631\n",
      "\n",
      " 632\n",
      "\n",
      " 633\n",
      "\n",
      " 634\n",
      "\n",
      " 635\n",
      "\n",
      " 636\n",
      "\n",
      " 637\n",
      "\n",
      " 638\n",
      "\n",
      " 639\n",
      "\n",
      " 640\n",
      "\n",
      " 641\n",
      "\n",
      " 642\n",
      "\n",
      " 643\n",
      "\n",
      " 644\n",
      "\n",
      " 645\n",
      "\n",
      " 646\n",
      "\n",
      " 647\n",
      "\n",
      " 648\n",
      "\n",
      " 649\n",
      "\n",
      " 650\n",
      "\n",
      " 651\n",
      "\n",
      " 652\n",
      "\n",
      " 653\n",
      "\n",
      " 654\n",
      "\n",
      " 655\n",
      "\n",
      " 656\n",
      "\n",
      " 657\n",
      "\n",
      " 658\n",
      "\n",
      " 659\n",
      "\n",
      " 660\n",
      "\n",
      " 661\n",
      "\n",
      " 662\n",
      "\n",
      " 663\n",
      "\n",
      " 664\n",
      "\n",
      " 665\n",
      "\n",
      " 666\n",
      "\n",
      " 667\n",
      "\n",
      " 668\n",
      "\n",
      " 669\n",
      "\n",
      " 670\n",
      "\n",
      " 671\n",
      "\n",
      " 672\n",
      "\n",
      " 673\n",
      "\n",
      " 674\n",
      "\n",
      " 675\n",
      "\n",
      " 676\n",
      "\n",
      " 677\n",
      "\n",
      " 678\n",
      "\n",
      " 679\n",
      "\n",
      " 680\n",
      "\n",
      " 681\n",
      "\n",
      " 682\n",
      "\n",
      " 683\n",
      "\n",
      " 684\n",
      "\n",
      " 685\n",
      "\n",
      " 686\n",
      "\n",
      " 687\n",
      "\n",
      " 688\n",
      "\n",
      " 689\n",
      "\n",
      " 690\n",
      "\n",
      " 691\n",
      "\n",
      " 692\n",
      "\n",
      " 693\n",
      "\n",
      " 694\n",
      "\n",
      " 695\n",
      "\n",
      " 696\n",
      "\n",
      " 697\n",
      "\n",
      " 698\n",
      "\n",
      " 699\n",
      "\n",
      " 700\n",
      "\n",
      " 701\n",
      "\n",
      " 702\n",
      "\n",
      " 703\n",
      "\n",
      " 704\n",
      "\n",
      " 705\n",
      "\n",
      " 706\n",
      "\n",
      " 707\n",
      "\n",
      " 708\n",
      "\n",
      " 709\n",
      "\n",
      " 710\n",
      "\n",
      " 711\n",
      "\n",
      " 712\n",
      "\n",
      " 713\n",
      "\n",
      " 714\n",
      "\n",
      " 715\n",
      "\n",
      " 716\n",
      "\n",
      " 717\n",
      "\n",
      " 718\n",
      "\n",
      " 719\n",
      "\n",
      " 720\n",
      "\n",
      " 721\n",
      "\n",
      " 722\n",
      "\n",
      " 723\n",
      "\n",
      " 724\n",
      "\n",
      " 725\n",
      "\n",
      " 726\n",
      "\n",
      " 727\n",
      "\n",
      " 728\n",
      "\n",
      " 729\n",
      "\n",
      " 730\n",
      "\n",
      " 731\n",
      "\n",
      " 732\n",
      "\n",
      " 733\n",
      "\n",
      " 734\n",
      "\n",
      " 735\n",
      "\n",
      " 736\n",
      "\n",
      " 737\n",
      "\n",
      " 738\n",
      "\n",
      " 739\n",
      "\n",
      " 740\n",
      "\n",
      " 741\n",
      "\n",
      " 742\n",
      "\n",
      " 743\n",
      "\n",
      " 744\n",
      "\n",
      " 745\n",
      "\n",
      " 746\n",
      "\n",
      " 747\n",
      "\n",
      " 748\n",
      "\n",
      " 749\n",
      "\n",
      " 750\n",
      "\n",
      " 751\n",
      "\n",
      " 752\n",
      "\n",
      " 753\n",
      "\n",
      " 754\n",
      "\n",
      " 755\n",
      "\n",
      " 756\n",
      "\n",
      " 757\n",
      "\n",
      " 758\n",
      "\n",
      " 759\n",
      "\n",
      " 760\n",
      "\n",
      " 761\n",
      "\n",
      " 762\n",
      "\n",
      " 763\n",
      "\n",
      " 764\n",
      "\n",
      " 765\n",
      "\n",
      " 766\n",
      "\n",
      " 767\n",
      "\n",
      " 768\n",
      "\n",
      " 769\n",
      "\n",
      " 770\n",
      "\n",
      " 771\n",
      "\n",
      " 772\n",
      "\n",
      " 773\n",
      "\n",
      " 774\n",
      "\n",
      " 775\n",
      "\n",
      " 776\n",
      "\n",
      " 777\n",
      "\n",
      " 778\n",
      "\n",
      " 779\n",
      "\n",
      " 780\n",
      "\n",
      " 781\n",
      "\n",
      " 782\n",
      "\n",
      " 783\n",
      "\n",
      " 784\n",
      "\n",
      " 785\n",
      "\n",
      " 786\n",
      "\n",
      " 787\n",
      "\n",
      " 788\n",
      "\n",
      " 789\n",
      "\n",
      " 790\n",
      "\n",
      " 791\n",
      "\n",
      " 792\n",
      "\n",
      " 793\n",
      "\n",
      " 794\n",
      "\n",
      " 795\n",
      "\n",
      " 796\n",
      "\n",
      " 797\n",
      "\n",
      " 798\n",
      "\n",
      " 799\n",
      "\n",
      " 800\n",
      "\n",
      " 801\n",
      "\n",
      " 802\n",
      "\n",
      " 803\n",
      "\n",
      " 804\n",
      "\n",
      " 805\n",
      "\n",
      " 806\n",
      "\n",
      " 807\n",
      "\n",
      " 808\n",
      "\n",
      " 809\n",
      "\n",
      " 810\n",
      "\n",
      " 811\n",
      "\n",
      " 812\n",
      "\n",
      " 813\n",
      "\n",
      " 814\n",
      "\n",
      " 815\n",
      "\n",
      " 816\n",
      "\n",
      " 817\n",
      "\n",
      " 818\n",
      "\n",
      " 819\n",
      "\n",
      " 820\n",
      "\n",
      " 821\n",
      "\n",
      " 822\n",
      "\n",
      " 823\n",
      "\n",
      " 824\n",
      "\n",
      " 825\n",
      "\n",
      " 826\n",
      "\n",
      " 827\n",
      "\n",
      " 828\n",
      "\n",
      " 829\n",
      "\n",
      " 830\n",
      "\n",
      " 831\n",
      "\n",
      " 832\n",
      "\n",
      " 833\n",
      "\n",
      " 834\n",
      "\n",
      " 835\n",
      "\n",
      " 836\n",
      "\n",
      " 837\n",
      "\n",
      " 838\n",
      "\n",
      " 839\n",
      "\n",
      " 840\n",
      "\n",
      " 841\n",
      "\n",
      " 842\n",
      "\n",
      " 843\n",
      "\n",
      " 844\n",
      "\n",
      " 845\n",
      "\n",
      " 846\n",
      "\n",
      " 847\n",
      "\n",
      " 848\n",
      "\n",
      " 849\n",
      "\n",
      " 850\n",
      "\n",
      " 851\n",
      "\n",
      " 852\n",
      "\n",
      " 853\n",
      "\n",
      " 854\n",
      "\n",
      " 855\n",
      "\n",
      " 856\n",
      "\n",
      " 857\n",
      "\n",
      " 858\n",
      "\n",
      " 859\n",
      "\n",
      " 860\n",
      "\n",
      " 861\n",
      "\n",
      " 862\n",
      "\n",
      " 863\n",
      "\n",
      " 864\n",
      "\n",
      " 865\n",
      "\n",
      " 866\n",
      "\n",
      " 867\n",
      "\n",
      " 868\n",
      "\n",
      " 869\n",
      "\n",
      " 870\n",
      "\n",
      " 871\n",
      "\n",
      " 872\n",
      "\n",
      " 873\n",
      "\n",
      " 874\n",
      "\n",
      " 875\n",
      "\n",
      " 876\n",
      "\n",
      " 877\n",
      "\n",
      " 878\n",
      "\n",
      " 879\n",
      "\n",
      " 880\n",
      "\n",
      " 881\n",
      "\n",
      " 882\n",
      "\n",
      " 883\n",
      "\n",
      " 884\n",
      "\n",
      " 885\n",
      "\n",
      " 886\n",
      "\n",
      " 887\n",
      "\n",
      " 888\n",
      "\n",
      " 889\n",
      "\n",
      " 890\n",
      "\n",
      " 891\n",
      "\n",
      " 892\n",
      "\n",
      " 893\n",
      "\n",
      " 894\n",
      "\n",
      " 895\n",
      "\n",
      " 896\n",
      "\n",
      " 897\n",
      "\n",
      " 898\n",
      "\n",
      " 899\n",
      "\n",
      " 900\n",
      "\n",
      " 901\n",
      "\n",
      " 902\n",
      "\n",
      " 903\n",
      "\n",
      " 904\n",
      "\n",
      " 905\n",
      "\n",
      " 906\n",
      "\n",
      " 907\n",
      "\n",
      " 908\n",
      "\n",
      " 909\n",
      "\n",
      " 910\n",
      "\n",
      " 911\n",
      "\n",
      " 912\n",
      "\n",
      " 913\n",
      "\n",
      " 914\n",
      "\n",
      " 915\n",
      "\n",
      " 916\n",
      "\n",
      " 917\n",
      "\n",
      " 918\n",
      "\n",
      " 919\n",
      "\n",
      " 920\n",
      "\n",
      " 921\n",
      "\n",
      " 922\n",
      "\n",
      " 923\n",
      "\n",
      " 924\n",
      "\n",
      " 925\n",
      "\n",
      " 926\n",
      "\n",
      " 927\n",
      "\n",
      " 928\n",
      "\n",
      " 929\n",
      "\n",
      " 930\n",
      "\n",
      " 931\n",
      "\n",
      " 932\n",
      "\n",
      " 933\n",
      "\n",
      " 934\n",
      "\n",
      " 935\n",
      "\n",
      " 936\n",
      "\n",
      " 937\n",
      "\n",
      " 938\n",
      "\n",
      " 939\n",
      "\n",
      " 940\n",
      "\n",
      " 941\n",
      "\n",
      " 942\n",
      "\n",
      " 943\n",
      "\n",
      " 944\n",
      "\n",
      " 945\n",
      "\n",
      " 946\n",
      "\n",
      " 947\n",
      "\n",
      " 948\n",
      "\n",
      " 949\n",
      "\n",
      " 950\n",
      "\n",
      " 951\n",
      "\n",
      " 952\n",
      "\n",
      " 953\n",
      "\n",
      " 954\n",
      "\n",
      " 955\n",
      "\n",
      " 956\n",
      "\n",
      " 957\n",
      "\n",
      " 958\n",
      "\n",
      " 959\n",
      "\n",
      " 960\n",
      "\n",
      " 961\n",
      "\n",
      " 962\n",
      "\n",
      " 963\n",
      "\n",
      " 964\n",
      "\n",
      " 965\n",
      "\n",
      " 966\n",
      "\n",
      " 967\n",
      "\n",
      " 968\n",
      "\n",
      " 969\n",
      "\n",
      " 970\n",
      "\n",
      " 971\n",
      "\n",
      " 972\n",
      "\n",
      " 973\n",
      "\n",
      " 974\n",
      "\n",
      " 975\n",
      "\n",
      " 976\n",
      "\n",
      " 977\n",
      "\n",
      " 978\n",
      "\n",
      " 979\n",
      "\n",
      " 980\n",
      "\n",
      " 981\n",
      "\n",
      " 982\n",
      "\n",
      " 983\n",
      "\n",
      " 984\n",
      "\n",
      " 985\n",
      "\n",
      " 986\n",
      "\n",
      " 987\n",
      "\n",
      " 988\n",
      "\n",
      " 989\n",
      "\n",
      " 990\n",
      "\n",
      " 991\n",
      "\n",
      " 992\n",
      "\n",
      " 993\n",
      "\n",
      " 994\n",
      "\n",
      " 995\n",
      "\n",
      " 996\n",
      "\n",
      " 997\n",
      "\n",
      " 998\n",
      "\n",
      " 999\n",
      "\n",
      " 1000\n",
      "\n",
      " 1001\n",
      "\n",
      " 1002\n",
      "\n",
      " 1003\n",
      "\n",
      " 1004\n",
      "\n",
      " 1005\n",
      "\n",
      " 1006\n",
      "\n",
      " 1007\n",
      "\n",
      " 1008\n",
      "\n",
      " 1009\n",
      "\n",
      " 1010\n",
      "\n",
      " 1011\n",
      "\n",
      " 1012\n",
      "\n",
      " 1013\n",
      "\n",
      " 1014\n",
      "\n",
      " 1015\n",
      "\n",
      " 1016\n",
      "\n",
      " 1017\n",
      "\n",
      " 1018\n",
      "\n",
      " 1019\n",
      "\n",
      " 1020\n",
      "\n",
      " 1021\n",
      "\n",
      " 1022\n",
      "\n",
      " 1023\n",
      "\n",
      " 1024\n",
      "\n",
      " 1025\n",
      "\n",
      " 1026\n",
      "\n",
      " 1027\n",
      "\n",
      " 1028\n",
      "\n",
      " 1029\n",
      "\n",
      " 1030\n",
      "\n",
      " 1031\n",
      "\n",
      " 1032\n",
      "\n",
      " 1033\n",
      "\n",
      " 1034\n",
      "\n",
      " 1035\n",
      "\n",
      " 1036\n",
      "\n",
      " 1037\n",
      "\n",
      " 1038\n",
      "\n",
      " 1039\n",
      "\n",
      " 1040\n",
      "\n",
      " 1041\n",
      "\n",
      " 1042\n",
      "\n",
      " 1043\n",
      "\n",
      " 1044\n",
      "\n",
      " 1045\n",
      "\n",
      " 1046\n",
      "\n",
      " 1047\n",
      "\n",
      " 1048\n",
      "\n",
      " 1049\n",
      "\n",
      " 1050\n",
      "\n",
      " 1051\n",
      "\n",
      " 1052\n",
      "\n",
      " 1053\n",
      "\n",
      " 1054\n",
      "\n",
      " 1055\n",
      "\n",
      " 1056\n",
      "\n",
      " 1057\n",
      "\n",
      " 1058\n",
      "\n",
      " 1059\n",
      "\n",
      " 1060\n",
      "\n",
      " 1061\n",
      "\n",
      " 1062\n",
      "\n",
      " 1063\n",
      "\n",
      " 1064\n",
      "\n",
      " 1065\n",
      "\n",
      " 1066\n",
      "\n",
      " 1067\n",
      "\n",
      " 1068\n",
      "\n",
      " 1069\n",
      "\n",
      " 1070\n",
      "\n",
      " 1071\n",
      "\n",
      " 1072\n",
      "\n",
      " 1073\n",
      "\n",
      " 1074\n",
      "\n",
      " 1075\n",
      "\n",
      " 1076\n",
      "\n",
      " 1077\n",
      "\n",
      " 1078\n",
      "\n",
      " 1079\n",
      "\n",
      " 1080\n",
      "\n",
      " 1081\n",
      "\n",
      " 1082\n",
      "\n",
      " 1083\n",
      "\n",
      " 1084\n",
      "\n",
      " 1085\n",
      "\n",
      " 1086\n",
      "\n",
      " 1087\n",
      "\n",
      " 1088\n",
      "\n",
      " 1089\n",
      "\n",
      " 1090\n",
      "\n",
      " 1091\n",
      "\n",
      " 1092\n",
      "\n",
      " 1093\n",
      "\n",
      " 1094\n",
      "\n",
      " 1095\n",
      "\n",
      " 1096\n",
      "\n",
      " 1097\n",
      "\n",
      " 1098\n",
      "\n",
      " 1099\n",
      "\n",
      " 1100\n",
      "\n",
      " 1101\n",
      "\n",
      " 1102\n",
      "\n",
      " 1103\n",
      "\n",
      " 1104\n",
      "\n",
      " 1105\n",
      "\n",
      " 1106\n",
      "\n",
      " 1107\n",
      "\n",
      " 1108\n",
      "\n",
      " 1109\n",
      "\n",
      " 1110\n",
      "\n",
      " 1111\n",
      "\n",
      " 1112\n",
      "\n",
      " 1113\n",
      "\n",
      " 1114\n",
      "\n",
      " 1115\n",
      "\n",
      " 1116\n",
      "\n",
      " 1117\n",
      "\n",
      " 1118\n",
      "\n",
      " 1119\n",
      "\n",
      " 1120\n",
      "\n",
      " 1121\n",
      "\n",
      " 1122\n",
      "\n",
      " 1123\n",
      "\n",
      " 1124\n",
      "\n",
      " 1125\n",
      "\n",
      " 1126\n",
      "\n",
      " 1127\n",
      "\n",
      " 1128\n",
      "\n",
      " 1129\n",
      "\n",
      " 1130\n",
      "\n",
      " 1131\n",
      "\n",
      " 1132\n",
      "\n",
      " 1133\n",
      "\n",
      " 1134\n",
      "\n",
      " 1135\n",
      "\n",
      " 1136\n",
      "\n",
      " 1137\n",
      "\n",
      " 1138\n",
      "\n",
      " 1139\n",
      "\n",
      " 1140\n",
      "\n",
      " 1141\n",
      "\n",
      " 1142\n",
      "\n",
      " 1143\n",
      "\n",
      " 1144\n",
      "\n",
      " 1145\n",
      "\n",
      " 1146\n",
      "\n",
      " 1147\n",
      "\n",
      " 1148\n",
      "\n",
      " 1149\n",
      "\n",
      " 1150\n",
      "\n",
      " 1151\n",
      "\n",
      " 1152\n",
      "\n",
      " 1153\n",
      "\n",
      " 1154\n",
      "\n",
      " 1155\n",
      "\n",
      " 1156\n",
      "\n",
      " 1157\n",
      "\n",
      " 1158\n",
      "\n",
      " 1159\n",
      "\n",
      " 1160\n",
      "\n",
      " 1161\n",
      "\n",
      " 1162\n",
      "\n",
      " 1163\n",
      "\n",
      " 1164\n",
      "\n",
      " 1165\n",
      "\n",
      " 1166\n",
      "\n",
      " 1167\n",
      "\n",
      " 1168\n",
      "\n",
      " 1169\n",
      "\n",
      " 1170\n",
      "\n",
      " 1171\n",
      "\n",
      " 1172\n",
      "\n",
      " 1173\n",
      "\n",
      " 1174\n",
      "\n",
      " 1175\n",
      "\n",
      " 1176\n",
      "\n",
      " 1177\n",
      "\n",
      " 1178\n",
      "\n",
      " 1179\n",
      "\n",
      " 1180\n",
      "\n",
      " 1181\n",
      "\n",
      " 1182\n",
      "\n",
      " 1183\n",
      "\n",
      " 1184\n",
      "\n",
      " 1185\n",
      "\n",
      " 1186\n",
      "\n",
      " 1187\n",
      "\n",
      " 1188\n",
      "\n",
      " 1189\n",
      "\n",
      " 1190\n",
      "\n",
      " 1191\n",
      "\n",
      " 1192\n",
      "\n",
      " 1193\n",
      "\n",
      " 1194\n",
      "\n",
      " 1195\n",
      "\n",
      " 1196\n",
      "\n",
      " 1197\n",
      "\n",
      " 1198\n",
      "\n",
      " 1199\n",
      "\n",
      " 1200\n",
      "\n",
      " 1201\n",
      "\n",
      " 1202\n",
      "\n",
      " 1203\n",
      "\n",
      " 1204\n",
      "\n",
      " 1205\n",
      "\n",
      " 1206\n",
      "\n",
      " 1207\n",
      "\n",
      " 1208\n",
      "\n",
      " 1209\n",
      "\n",
      " 1210\n",
      "\n",
      " 1211\n",
      "\n",
      " 1212\n",
      "\n",
      " 1213\n",
      "\n",
      " 1214\n",
      "\n",
      " 1215\n",
      "\n",
      " 1216\n",
      "\n",
      " 1217\n",
      "\n",
      " 1218\n",
      "\n",
      " 1219\n",
      "\n",
      " 1220\n",
      "\n",
      " 1221\n",
      "\n",
      " 1222\n",
      "\n",
      " 1223\n",
      "\n",
      " 1224\n",
      "\n",
      " 1225\n",
      "\n",
      " 1226\n",
      "\n",
      " 1227\n",
      "\n",
      " 1228\n",
      "\n",
      " 1229\n",
      "\n",
      " 1230\n",
      "\n",
      " 1231\n",
      "\n",
      " 1232\n",
      "\n",
      " 1233\n",
      "\n",
      " 1234\n",
      "\n",
      " 1235\n",
      "\n",
      " 1236\n",
      "\n",
      " 1237\n",
      "\n",
      " 1238\n",
      "\n",
      " 1239\n",
      "\n",
      " 1240\n",
      "\n",
      " 1241\n",
      "\n",
      " 1242\n",
      "\n",
      " 1243\n",
      "\n",
      " 1244\n",
      "\n",
      " 1245\n",
      "\n",
      " 1246\n",
      "\n",
      " 1247\n",
      "\n",
      " 1248\n",
      "\n",
      " 1249\n",
      "\n",
      " 1250\n",
      "\n",
      " 1251\n",
      "\n",
      " 1252\n",
      "\n",
      " 1253\n",
      "\n",
      " 1254\n",
      "\n",
      " 1255\n",
      "\n",
      " 1256\n",
      "\n",
      " 1257\n",
      "\n",
      " 1258\n",
      "\n",
      " 1259\n",
      "\n",
      " 1260\n",
      "\n",
      " 1261\n",
      "\n",
      " 1262\n",
      "\n",
      " 1263\n",
      "\n",
      " 1264\n",
      "\n",
      " 1265\n",
      "\n",
      " 1266\n",
      "\n",
      " 1267\n",
      "\n",
      " 1268\n",
      "\n",
      " 1269\n",
      "\n",
      " 1270\n",
      "\n",
      " 1271\n",
      "\n",
      " 1272\n",
      "\n",
      " 1273\n",
      "\n",
      " 1274\n",
      "\n",
      " 1275\n",
      "\n",
      " 1276\n",
      "\n",
      " 1277\n",
      "\n",
      " 1278\n",
      "\n",
      " 1279\n",
      "\n",
      " 1280\n",
      "\n",
      " 1281\n",
      "\n",
      " 1282\n",
      "\n",
      " 1283\n",
      "\n",
      " 1284\n",
      "\n",
      " 1285\n",
      "\n",
      " 1286\n",
      "\n",
      " 1287\n",
      "\n",
      " 1288\n",
      "\n",
      " 1289\n",
      "\n",
      " 1290\n",
      "\n",
      " 1291\n",
      "\n",
      " 1292\n",
      "\n",
      " 1293\n",
      "\n",
      " 1294\n",
      "\n",
      " 1295\n",
      "\n",
      " 1296\n",
      "\n",
      " 1297\n",
      "\n",
      " 1298\n",
      "\n",
      " 1299\n",
      "\n",
      " 1300\n",
      "\n",
      " 1301\n",
      "\n",
      " 1302\n",
      "\n",
      " 1303\n",
      "\n",
      " 1304\n",
      "\n",
      " 1305\n",
      "\n",
      " 1306\n",
      "\n",
      " 1307\n",
      "\n",
      " 1308\n",
      "\n",
      " 1309\n",
      "\n",
      " 1310\n",
      "\n",
      " 1311\n",
      "\n",
      " 1312\n",
      "\n",
      " 1313\n",
      "\n",
      " 1314\n",
      "\n",
      " 1315\n",
      "\n",
      " 1316\n",
      "\n",
      " 1317\n",
      "\n",
      " 1318\n",
      "\n",
      " 1319\n",
      "\n",
      " 1320\n",
      "\n",
      " 1321\n",
      "\n",
      " 1322\n",
      "\n",
      " 1323\n",
      "\n",
      " 1324\n",
      "\n",
      " 1325\n",
      "\n",
      " 1326\n",
      "\n",
      " 1327\n",
      "\n",
      " 1328\n",
      "\n",
      " 1329\n",
      "\n",
      " 1330\n",
      "\n",
      " 1331\n",
      "\n",
      " 1332\n",
      "\n",
      " 1333\n",
      "\n",
      " 1334\n",
      "\n",
      " 1335\n",
      "\n",
      " 1336\n",
      "\n",
      " 1337\n",
      "\n",
      " 1338\n",
      "\n",
      " 1339\n",
      "\n",
      " 1340\n",
      "\n",
      " 1341\n",
      "\n",
      " 1342\n",
      "\n",
      " 1343\n",
      "\n",
      " 1344\n",
      "\n",
      " 1345\n",
      "\n",
      " 1346\n",
      "\n",
      " 1347\n",
      "\n",
      " 1348\n",
      "\n",
      " 1349\n",
      "\n",
      " 1350\n",
      "\n",
      " 1351\n",
      "\n",
      " 1352\n",
      "\n",
      " 1353\n",
      "\n",
      " 1354\n",
      "\n",
      " 1355\n",
      "\n",
      " 1356\n",
      "\n",
      " 1357\n",
      "\n",
      " 1358\n",
      "\n",
      " 1359\n",
      "\n",
      " 1360\n",
      "\n",
      " 1361\n",
      "\n",
      " 1362\n",
      "\n",
      " 1363\n",
      "\n",
      " 1364\n",
      "\n",
      " 1365\n",
      "\n",
      " 1366\n",
      "\n",
      " 1367\n",
      "\n",
      " 1368\n",
      "\n",
      " 1369\n",
      "\n",
      " 1370\n",
      "\n",
      " 1371\n",
      "\n",
      " 1372\n",
      "\n",
      " 1373\n",
      "\n",
      " 1374\n",
      "\n",
      " 1375\n",
      "\n",
      " 1376\n",
      "\n",
      " 1377\n",
      "\n",
      " 1378\n",
      "\n",
      " 1379\n",
      "\n",
      " 1380\n",
      "\n",
      " 1381\n",
      "\n",
      " 1382\n",
      "\n",
      " 1383\n",
      "\n",
      " 1384\n",
      "\n",
      " 1385\n",
      "\n",
      " 1386\n",
      "\n",
      " 1387\n",
      "\n",
      " 1388\n",
      "\n",
      " 1389\n",
      "\n",
      " 1390\n",
      "\n",
      " 1391\n",
      "\n",
      " 1392\n",
      "\n",
      " 1393\n",
      "\n",
      " 1394\n",
      "\n",
      " 1395\n",
      "\n",
      " 1396\n",
      "\n",
      " 1397\n",
      "\n",
      " 1398\n",
      "\n",
      " 1399\n",
      "\n",
      " 1400\n",
      "\n",
      " 1401\n",
      "\n",
      " 1402\n",
      "\n",
      " 1403\n",
      "\n",
      " 1404\n",
      "\n",
      " 1405\n",
      "\n",
      " 1406\n",
      "\n",
      " 1407\n",
      "\n",
      " 1408\n",
      "\n",
      " 1409\n",
      "\n",
      " 1410\n",
      "\n",
      " 1411\n",
      "\n",
      " 1412\n",
      "\n",
      " 1413\n",
      "\n",
      " 1414\n",
      "\n",
      " 1415\n",
      "\n",
      " 1416\n",
      "\n",
      " 1417\n",
      "\n",
      " 1418\n",
      "\n",
      " 1419\n",
      "\n",
      " 1420\n",
      "\n",
      " 1421\n",
      "\n",
      " 1422\n",
      "\n",
      " 1423\n",
      "\n",
      " 1424\n",
      "\n",
      " 1425\n",
      "\n",
      " 1426\n",
      "\n",
      " 1427\n",
      "\n",
      " 1428\n",
      "\n",
      " 1429\n",
      "\n",
      " 1430\n",
      "\n",
      " 1431\n",
      "\n",
      " 1432\n",
      "\n",
      " 1433\n",
      "\n",
      " 1434\n",
      "\n",
      " 1435\n",
      "\n",
      " 1436\n",
      "\n",
      " 1437\n",
      "\n",
      " 1438\n",
      "\n",
      " 1439\n",
      "\n",
      " 1440\n"
     ]
    }
   ],
   "source": [
    "index = range(1,5)\n",
    "Parameter_df = pd.DataFrame({\"Numb Features\" : 0.1 , \"Numb Trees\": 0.2, \"Min Leaf Size\":0.2, \"Node Split\": 0.3,\"CV\":0.4},\n",
    "                            index= index)\n",
    "\n",
    "%%timeit \n",
    "i = 0 # Track number of iterations and for indexing information \n",
    "for num_est in range(4,10,2):\n",
    "    for num_feat in range(1,(len(X.columns)-1)):\n",
    "        for min_leaf in range(1,20,5):\n",
    "            for min_split in range(2,102,20):\n",
    "                RF_PT = RandomForestClassifier(n_estimators = num_est, max_features = num_feat, min_samples_split= min_split\n",
    "                                           ,min_samples_leaf = min_leaf, random_state=1)\n",
    "                RF_PT.fit(X,Y)\n",
    "                predict_RF_PT1 = RF.predict(X)\n",
    "                CV = cross_val_score(RF_PT, X,Y,cv= 5).mean()\n",
    "                Parameter_df.loc[i] = [num_feat, num_est, min_leaf, min_split ,CV]\n",
    "                i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CV  Min Leaf Size  Node Split  Numb Features  Numb Trees\n",
      "778   5.0            6.0        16.0           82.0    0.643785\n",
      "1033  2.0            8.0         4.0           32.0    0.642584\n",
      "1057  2.0            8.0        10.0           72.0    0.641860\n",
      "1067  2.0            8.0        13.0           72.0    0.641852\n",
      "553   2.0            6.0         4.0           32.0    0.640901\n",
      "1 loop, best of 1: 6.55 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "len(Parameter_df)\n",
    "Parameter_df2 = Parameter_df.sort_values(by = \"Numb Trees\", ascending = False)\n",
    "print(Parameter_df2.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lets try ensembling with a Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('rf', RandomF...owski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'))],\n",
       "         n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eclf1 = VotingClassifier(estimators = [('lr', log_fit), ('rf', RF), ('knn', KNN)], voting = 'hard')\n",
    "eclf2 = VotingClassifier(estimators = [('lr', log_fit), ('rf', RF), ('knn', KNN)], voting = 'soft')\n",
    "\n",
    "eclf1.fit(X,Y)\n",
    "eclf2.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.80      0.93      0.86      1407\n",
      "          2       0.83      0.74      0.78      1323\n",
      "          3       0.86      0.82      0.84      1447\n",
      "\n",
      "avg / total       0.83      0.83      0.83      4177\n",
      "\n",
      "[[1303   83   21]\n",
      " [ 178  973  172]\n",
      " [ 151  110 1186]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.90      0.94      0.92      1407\n",
      "          2       0.91      0.84      0.87      1323\n",
      "          3       0.91      0.93      0.92      1447\n",
      "\n",
      "avg / total       0.90      0.90      0.90      4177\n",
      "\n",
      "[[1316   60   31]\n",
      " [ 101 1115  107]\n",
      " [  51   53 1343]]\n"
     ]
    }
   ],
   "source": [
    "eclf1_predict = eclf1.predict(X)\n",
    "eclf2_predict = eclf2.predict(X)\n",
    "\n",
    "print(metrics.classification_report(Y,eclf1_predict))\n",
    "print(metrics.confusion_matrix(Y, eclf1_predict))\n",
    "\n",
    "print(metrics.classification_report(Y,eclf2_predict))\n",
    "print(metrics.confusion_matrix(Y, eclf2_predict))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let begin using cross validation on the different algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_alg = {log_fit, RF, KNN, eclf1, eclf2}\n",
    "scores = {}\n",
    "score_mean = {}\n",
    "score_sd = {}\n",
    "for alg in all_alg:\n",
    "    scores[alg] = cross_val_score(alg, X, Y, cv = 10);\n",
    "    score_mean[alg] = scores[alg].mean()\n",
    "    score_sd[alg] = scores[alg].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Statistics \n",
      "\n",
      "Mean: 0.6411 Standard Deviation: 0.0260 \n",
      "Mean: 0.6148 Standard Deviation: 0.0291 \n",
      "Mean: 0.6294 Standard Deviation: 0.0233 \n",
      "Mean: 0.6134 Standard Deviation: 0.0193 \n",
      "Mean: 0.6220 Standard Deviation: 0.0342 \n"
     ]
    }
   ],
   "source": [
    "print(\"Cross Validation Statistics \\n\")\n",
    "for alg in all_alg:\n",
    "    print(\"Mean: %0.4f Standard Deviation: %0.4f \" % (scores[alg].mean(), scores[alg].std()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's try looking into the correlation structure of the sections of the data sets \n",
    "We will have to remove the sex class, seperate by age category then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   Shell weight  \n",
       "0         0.150  \n",
       "1         0.070  \n",
       "2         0.210  \n",
       "3         0.155  \n",
       "4         0.055  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_data =  data_use.drop(['Sex', 'AgeCategory'],1)\n",
    "pca_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca1 = PCA(n_components=7)\n",
    "pca1.fit(pca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.74100729e-01   1.14183886e-02   8.37567021e-03   3.03865180e-03\n",
      "   1.41047670e-03   1.22936128e-03   4.26722250e-04]\n"
     ]
    }
   ],
   "source": [
    "pca_score = pca1.explained_variance_ratio_\n",
    "print(pca_score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First principal component explains 97% of the variance... We might only need that for the future analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.37297053],\n",
       "       [-0.72693017],\n",
       "       [-0.17700541],\n",
       "       ..., \n",
       "       [ 0.41895149],\n",
       "       [ 0.34791783],\n",
       "       [ 1.31843776]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca1_comp = PCA(n_components=1)\n",
    "pca_data1 = pca1_comp.fit_transform(pca_data)\n",
    "pca_data1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create a pipeline to run all the classic Classification algorithms and get the accuracy number and CV scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(random_state=1)\n",
    "RF2 = RandomForestClassifier(random_state=1)\n",
    "KNN2 = KNeighborsClassifier()\n",
    "ensemble1 = VotingClassifier(estimators = [('lr', logistic), ('rf', RF2), ('knn', KNN2)], voting = 'hard')\n",
    "ensemble2 = VotingClassifier(estimators = [('lr', logistic), ('rf', RF2), ('knn', KNN2)], voting = 'soft')\n",
    "\n",
    "alg_total2 = {logistic, RF2, KNN2, ensemble1, ensemble2}\n",
    "names = {\"Logistic Regression\", \"Random Forest\" ,\"KNN\", \"Ensemble Hard\", \"Ensemble Soft\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.83      0.91      0.87      1407\n",
      "          2       0.89      0.74      0.81      1323\n",
      "          3       0.83      0.87      0.85      1447\n",
      "\n",
      "avg / total       0.85      0.84      0.84      4177\n",
      "\n",
      "[[1280   51   76]\n",
      " [ 154  982  187]\n",
      " [ 110   73 1264]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.61      0.83      0.70      1407\n",
      "          2       0.36      0.06      0.10      1323\n",
      "          3       0.51      0.73      0.60      1447\n",
      "\n",
      "avg / total       0.50      0.55      0.48      4177\n",
      "\n",
      "[[1162   54  191]\n",
      " [ 423   77  823]\n",
      " [ 305   80 1062]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.68      0.84      0.75      1407\n",
      "          2       0.60      0.55      0.57      1323\n",
      "          3       0.70      0.60      0.65      1447\n",
      "\n",
      "avg / total       0.66      0.67      0.66      4177\n",
      "\n",
      "[[1183  148   76]\n",
      " [ 304  724  295]\n",
      " [ 244  329  874]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.69      0.95      0.80      1407\n",
      "          2       0.91      0.56      0.69      1323\n",
      "          3       0.78      0.78      0.78      1447\n",
      "\n",
      "avg / total       0.79      0.77      0.76      4177\n",
      "\n",
      "[[1335   19   53]\n",
      " [ 323  740  260]\n",
      " [ 268   56 1123]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.94      0.98      0.96      1407\n",
      "          2       0.94      0.94      0.94      1323\n",
      "          3       0.96      0.93      0.95      1447\n",
      "\n",
      "avg / total       0.95      0.95      0.95      4177\n",
      "\n",
      "[[1376   15   16]\n",
      " [  46 1243   34]\n",
      " [  43   62 1342]]\n"
     ]
    }
   ],
   "source": [
    "for alg2 in alg_total2:\n",
    "    alg2.fit(pca_data1, Y)\n",
    "    temp_predict = alg2.predict(pca_data1)\n",
    "    print(metrics.classification_report(Y,temp_predict))\n",
    "    print(metrics.confusion_matrix(Y, temp_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_pca = {}\n",
    "score_mean_pca = {}\n",
    "score_sd_pca = {}\n",
    "for alg in alg_total2:\n",
    "    scores_pca[alg] = cross_val_score(alg, pca_data1, Y, cv = 5);\n",
    "    score_mean_pca[alg] = scores_pca[alg].mean()\n",
    "    score_sd_pca[alg] = scores_pca[alg].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Statistics \n",
      "\n",
      "Mean: 0.5121 Standard Deviation: 0.0191 \n",
      "Mean: 0.5511 Standard Deviation: 0.0290 \n",
      "Mean: 0.5291 Standard Deviation: 0.0142 \n",
      "Mean: 0.5265 Standard Deviation: 0.0169 \n",
      "Mean: 0.4798 Standard Deviation: 0.0160 \n"
     ]
    }
   ],
   "source": [
    "print(\"Cross Validation Statistics \\n\")\n",
    "for alg in alg_total2:\n",
    "    print(\"Mean: %0.4f Standard Deviation: %0.4f \" % (scores_pca[alg].mean(), scores_pca[alg].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.94      0.98      0.96      1407\n",
      "          2       0.94      0.94      0.94      1323\n",
      "          3       0.96      0.93      0.95      1447\n",
      "\n",
      "avg / total       0.95      0.95      0.95      4177\n",
      "\n",
      "[[1376   15   16]\n",
      " [  46 1243   34]\n",
      " [  43   62 1342]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64136697801632492"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X,Y)\n",
    "lda_predict = lda.predict(X)\n",
    "print(metrics.classification_report(Y,temp_predict))\n",
    "print(metrics.confusion_matrix(Y, temp_predict))\n",
    "cross_val_score(lda,X,Y, cv = 5).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.94      0.98      0.96      1407\n",
      "          2       0.94      0.94      0.94      1323\n",
      "          3       0.96      0.93      0.95      1447\n",
      "\n",
      "avg / total       0.95      0.95      0.95      4177\n",
      "\n",
      "[[1376   15   16]\n",
      " [  46 1243   34]\n",
      " [  43   62 1342]]\n",
      "0.642570589205\n"
     ]
    }
   ],
   "source": [
    "clf1 = MLPClassifier()\n",
    "clf1.fit(X,Y)\n",
    "clf1_predict = clf1.predict(X)\n",
    "print(metrics.classification_report(Y,temp_predict))\n",
    "print(metrics.confusion_matrix(Y, temp_predict))\n",
    "print(cross_val_score(clf1, X,Y,cv= 5).mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's try some feature engineering and derive some features that might help \n",
    "Maybe the volume of the abalone might be useful- it gives the general size of the creature \n",
    "We also may try the weight* volume to give more information about the age- bigger and larger could mean older... \n",
    "\n",
    "for volume we will use the length* diameter* height \n",
    "for weight we can use the whole weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_FE = data_use\n",
    "data_FE[\"Volume\"] = data_use[\"Length\"]*data_use[\"Diameter\"]*data_use[\"Height\"]\n",
    "data_FE[\"Vol*Weight\"] = data_use[\"Volume\"] * data_use[\"Whole weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.67      0.79      0.73      1407\n",
      "          2       0.51      0.37      0.43      1323\n",
      "          3       0.66      0.71      0.68      1447\n",
      "\n",
      "avg / total       0.62      0.63      0.62      4177\n",
      " \n",
      " \n",
      " [[1115  231   61]\n",
      " [ 361  486  476]\n",
      " [ 186  231 1030]]\n",
      "\n",
      " Cross Validation score is 0.622 \n",
      " \n",
      "\n",
      "K- Nearest Neighbors \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.78      0.85      0.81      1407\n",
      "          2       0.64      0.66      0.65      1323\n",
      "          3       0.80      0.71      0.76      1447\n",
      "\n",
      "avg / total       0.74      0.74      0.74      4177\n",
      " \n",
      " \n",
      " [[1197  180   30]\n",
      " [ 226  872  225]\n",
      " [ 111  303 1033]]\n",
      "\n",
      " Cross Validation score is 0.610 \n",
      " \n",
      "\n",
      "Random Forest \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.99      0.99      0.99      1407\n",
      "          2       0.98      0.98      0.98      1323\n",
      "          3       0.99      0.98      0.99      1447\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4177\n",
      " \n",
      " \n",
      " [[1393   10    4]\n",
      " [  17 1290   16]\n",
      " [   3   20 1424]]\n",
      "\n",
      " Cross Validation score is 0.614 \n",
      " \n",
      "\n",
      "Linear Discriminant Analysis \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.74      0.74      0.74      1407\n",
      "          2       0.50      0.57      0.53      1323\n",
      "          3       0.72      0.63      0.67      1447\n",
      "\n",
      "avg / total       0.66      0.65      0.65      4177\n",
      " \n",
      " \n",
      " [[1041  329   37]\n",
      " [ 246  754  323]\n",
      " [ 114  419  914]]\n",
      "\n",
      " Cross Validation score is 0.638 \n",
      " \n",
      "\n",
      "Hard Vote Ensemble \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.76      0.90      0.83      1407\n",
      "          2       0.70      0.69      0.69      1323\n",
      "          3       0.85      0.70      0.77      1447\n",
      "\n",
      "avg / total       0.77      0.77      0.77      4177\n",
      " \n",
      " \n",
      " [[1272   99   36]\n",
      " [ 266  915  142]\n",
      " [ 128  301 1018]]\n",
      "\n",
      " Cross Validation score is 0.633 \n",
      " \n",
      "\n",
      "Soft Vote Ensemble \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.86      0.89      0.87      1407\n",
      "          2       0.83      0.80      0.81      1323\n",
      "          3       0.88      0.88      0.88      1447\n",
      "\n",
      "avg / total       0.86      0.86      0.86      4177\n",
      " \n",
      " \n",
      " [[1249  122   36]\n",
      " [ 128 1061  134]\n",
      " [  74  102 1271]]\n",
      "\n",
      " Cross Validation score is 0.642 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
